{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "This notebook is was built by Camille-Amaury JUGE in order to better understands the Q-Learning RL method. \n",
    "\n",
    "We will construct an agent which will try to find its path in a matrix (labyrinth) without walls, and then add complexity to it.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from ipycanvas import MultiCanvas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Labyrinth(object):\n",
    "    def __init__(self, x, y, max_reward, punition, final_pos, walls, wall_punition):\n",
    "        super(Labyrinth, self).__init__()\n",
    "        \n",
    "        # grid\n",
    "        self.lab = np.array([np.array([punition for j in range(y)]) for i in range(x)])\n",
    "        for i in range(self.lab.shape[0]):\n",
    "            for j in range(self.lab.shape[1]):\n",
    "                self.lab[0][1] = punition\n",
    "        self.lab[final_pos[0]][final_pos[1]] = max_reward\n",
    "        \n",
    "        # walls\n",
    "        self.walls = walls\n",
    "        self.wall_punition = wall_punition\n",
    "        \n",
    "        self.final_pos = final_pos\n",
    "        \n",
    "    def update_Pos(self, x, y, direction):\n",
    "        r = 0\n",
    "        if (y == 0 and direction == 0) or (y == self.lab.shape[1]-1 and direction == 1) or (x == 0 and direction == 2) or (x == self.lab.shape[0]-1 and direction == 3):\n",
    "            r = self.lab[final_pos[0]][final_pos[1]] if self.final_pos == (x,y) else self.wall_punition\n",
    "        else:\n",
    "            if self.walls[x + (-1 if direction == 2 else (1 if direction == 3 else 0))][y + (-1 if direction == 0 else (1 if direction == 1 else 0))]:\n",
    "                r = self.wall_punition\n",
    "            else:\n",
    "                y = y + (-1 if direction == 0 else (1 if direction == 1 else 0))\n",
    "                x = x + (-1 if direction == 2 else (1 if direction == 3 else 0))\n",
    "                r = self.lab[x][y]\n",
    "        return (x,y), r\n",
    "    \n",
    "    def add_punition_cell(self, x, y, punition):\n",
    "        self.lab[x][y] = punition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot(object):\n",
    "    def __init__(self, init_pos, lab, experiencing_rate, _decrease_rate, learning_rate):\n",
    "        super(Robot, self).__init__()\n",
    "        self.init_pos = init_pos\n",
    "        self.pos = self.init_pos\n",
    "        \n",
    "        self.actions = [\"Left\", \"Right\", \"Top\", \"Bottom\"]\n",
    "        self.labyrinth = lab\n",
    "        self.Q = np.zeros((self.labyrinth.lab.shape[0], self.labyrinth.lab.shape[1], len(self.actions)))\n",
    "        self.init_rates = [experiencing_rate, _decrease_rate, learning_rate]\n",
    "        self.rates = self.init_rates\n",
    "        self.history = []\n",
    "        \n",
    "    def train(self, epoch, long_term_importance):\n",
    "        for i in range(epoch):\n",
    "            self.pos = self.init_pos\n",
    "            \n",
    "            sys.stdout.write(\"\\repoch {} / {}\".format(i+1, epoch))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            while self.pos != self.labyrinth.final_pos:\n",
    "                action = self.choose_action()\n",
    "                Q_old = (self.pos[0], self.pos[1], action)\n",
    "                self.pos, reward = self.labyrinth.update_Pos(self.pos[0], self.pos[1], action)\n",
    "                self.update_Q(Q_old, self.pos, long_term_importance, reward)\n",
    "                \n",
    "            self.rates[0] -= self.rates[1]  \n",
    "            \n",
    "    def update_Q(self, q_old, pos_new, long_term_importance, reward):\n",
    "        Q_new = max(self.Q[pos_new[0]][pos_new[1]])\n",
    "        \n",
    "        #update following bellman's equation\n",
    "        self.Q[q_old[0]][q_old[1]][q_old[2]] += self.rates[2] * (reward + long_term_importance * Q_new - self.Q[q_old[0]][q_old[1]][q_old[2]])\n",
    "        \n",
    "    def choose_action(self):\n",
    "        # if experiencing then random decision\n",
    "        if np.random.uniform(0, 1) <= self.rates[0]:\n",
    "            action = np.random.randint(0,4)\n",
    "        # if using memory then maximize rewards\n",
    "        else:\n",
    "            action = np.argmax(self.Q[self.pos[0]][self.pos[1]])\n",
    "        return action\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    def __str__(self):\n",
    "        s = \"\"\n",
    "        for i, rows in enumerate(self.Q):\n",
    "            s += \"|\"\n",
    "            for j, column in enumerate(rows):\n",
    "                if self.labyrinth.final_pos == (i,j):\n",
    "                    s+=\"ðŸž¬|\"\n",
    "                else:\n",
    "                    if self.labyrinth.walls[i][j]:\n",
    "                        s+=\"â¯€|\"\n",
    "                    else:\n",
    "                        max_k = np.argmax(column)\n",
    "                        if max_k == 0:\n",
    "                            s += \"ðŸ¡„|\"\n",
    "                        elif max_k == 1:\n",
    "                            s += \"ðŸ¡†|\"\n",
    "                        elif max_k == 2:\n",
    "                            s += \"ðŸ¡…|\"\n",
    "                        elif max_k == 3:\n",
    "                            s += \"ðŸ¡‡|\"\n",
    "            s += \"\\n\"\n",
    "        return s\n",
    "    \n",
    "    def draw(self):\n",
    "        \n",
    "        _border = 2\n",
    "        _cell_size = 10\n",
    "        _size = (self.labyrinth.lab.shape[0] * _cell_size + 2 * _border, self.labyrinth.lab.shape[1] * _cell_size + 2 * _border)\n",
    "        \n",
    "        # Create a multi-layer canvas with 4 layers\n",
    "        canvas = MultiCanvas(3, width=_size[0], height=_size[1])\n",
    "        \n",
    "        # \n",
    "        for i, rows in enumerate(self.Q):\n",
    "            for j, column in enumerate(rows):\n",
    "                i_mod = j\n",
    "                j_mod = i\n",
    "                if self.labyrinth.final_pos == (i,j):\n",
    "                    self.draw_cell(canvas[0], i_mod, j_mod, _cell_size, _border, \"green\")\n",
    "                else:\n",
    "                    if self.labyrinth.walls[i][j]:\n",
    "                        self.draw_cell(canvas[0], i_mod, j_mod, _cell_size, _border, \"black\")\n",
    "                    else:\n",
    "                        if self.init_pos == (i,j):\n",
    "                            self.draw_cell(canvas[0], i_mod, j_mod, _cell_size, _border, \"gray\")\n",
    "                        direction = np.argmax(column)\n",
    "                        self.draw_arrow(canvas[1], i_mod, j_mod, _cell_size, _border, direction)\n",
    "        \n",
    "        canvas[2].fill_style = \"black\"\n",
    "        canvas[2].fill_rect(0, 0, _size[0], _border)\n",
    "        canvas[2].fill_rect(0, _size[1]-2, _size[0], _border)\n",
    "        canvas[2].fill_rect(0, 0, _border, _size[1])\n",
    "        canvas[2].fill_rect(_size[0]-2, 0, _border, _size[1])\n",
    "        \n",
    "        return canvas\n",
    "                            \n",
    "    \n",
    "    def draw_cell(self, canvas, x, y, size, border, color):\n",
    "        canvas.fill_style = color\n",
    "        canvas.fill_rect(x*size + border, y*size + border, size, size)\n",
    "        \n",
    "    def draw_arrow(self, canvas, x, y, size, border, direction):\n",
    "        canvas.fill_style = \"brown\"\n",
    "        canvas.begin_path()\n",
    "        if direction == 0:\n",
    "            canvas.move_to(border + x*size + 2, border + y*size + int(size/2))\n",
    "            canvas.line_to(border + x*size + 8, border + y*size + int(size/2) + 3)\n",
    "            canvas.line_to(border + x*size + 8, border + y*size + int(size/2) - 3)\n",
    "        elif direction == 1:\n",
    "            canvas.move_to(border + x*size + 8, border + y*size + int(size/2))\n",
    "            canvas.line_to(border + x*size + 2, border + y*size + int(size/2) + 3)\n",
    "            canvas.line_to(border + x*size + 2, border + y*size + int(size/2) - 3)\n",
    "        elif direction == 2:\n",
    "            canvas.move_to(border + x*size + int(size/2), border + y*size + 2)\n",
    "            canvas.line_to(border + x*size + int(size/2) + 3, border + y*size + 8)\n",
    "            canvas.line_to(border + x*size + int(size/2) - 3, border + y*size + 8)\n",
    "        elif direction == 3:\n",
    "            canvas.move_to(border + x*size + int(size/2), border + y*size + 8)\n",
    "            canvas.line_to(border + x*size + int(size/2) + 3, border + y*size + 2)\n",
    "            canvas.line_to(border + x*size + int(size/2) - 3, border + y*size + 2)\n",
    "        canvas.fill()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_final_position = (0,19)\n",
    "_init_position = (19,0)\n",
    "_lab_dim = (20,20)\n",
    "_rewards = (5,0,-20)\n",
    "# epsilon rate\n",
    "_experiencing_rate = 1\n",
    "_decrease_rate = 0.0005\n",
    "_learning_rate = 0.1\n",
    "_epochs = 2000\n",
    "long_term_importance = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "walls_1 = np.array([\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "    np.array([False,True,True,True,True,True,True,True,True,True,True,True,True,True,True,True,True,True,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([True,True,True,True,True,True,True,True,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,True,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False]),\n",
    "    np.array([True,True,True,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False]),\n",
    "    np.array([False,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,True,False]),\n",
    "    np.array([False,True,True,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_1 = Labyrinth(_lab_dim[0], _lab_dim[1], _rewards[0], _rewards[1], _final_position, walls_1, _rewards[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_1 = Robot(_init_position, lab_1, _experiencing_rate, _decrease_rate, _learning_rate)\n",
    "canvas = robot_1.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddf999e665742f09b8c3ee9632fde79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=204, width=204)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2000 / 2000\n",
      "function took 30.280 s\n"
     ]
    }
   ],
   "source": [
    "time1 = time.time()\n",
    "robot_1.train(_epochs, long_term_importance)\n",
    "time2 = time.time()\n",
    "print('\\nfunction took {:.3f} s'.format((time2-time1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "how to interpret :\n",
    "\n",
    "Arrows represent the way the agent tried to maximize the reward by reaching the green square.\n",
    "It begins on the grey square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f269755f30944c2b972bac9a364a3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=204, width=204)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "canvas = robot_1.draw()\n",
    "canvas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
